{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Regression Questions:-**"
      ],
      "metadata": {
        "id": "hr-1mEcVI1L3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **What is Simple Linear Regression?**\n",
        "  - Simple Linear Regression is a method that models the relationship between two variables by fitting a linear equation to the observed data.\n",
        "\n",
        "---\n",
        "\n",
        "2. **What are the key assumptions of Simple Linear Regression?**\n",
        "   - Linearity: The relationship between the independent and dependent variable is linear.\n",
        "   - Independence: The observations are independent of each other.\n",
        "   - Homoscedasticity: Constant variance of errors.\n",
        "   - Normality: The errors should be normally distributed.\n",
        "\n",
        "---\n",
        "\n",
        "3. **What does the coefficient m represent in the equation Y=mX+c?**\n",
        "  - The coefficient 'm' represents the slope of the regression line, indicating the change in Y for a one-unit change in X.\n",
        "\n",
        "---\n",
        "\n",
        "4. **What does the intercept c represent in the equation Y=mX+c?**\n",
        "  - The intercept 'c' represents the value of Y when X is zero, i.e., the point where the regression line crosses the Y-axis.\n",
        "\n",
        "---\n",
        "\n",
        "5. **How do we calculate the slope m in Simple Linear Regression?**\n",
        "  - The slope m can be calculated using the formula:\n",
        "   m = Σ[(Xi - X̄)(Yi - Ŷ)] / Σ(Xi - X̄)²\n",
        "\n",
        "---\n",
        "\n",
        "6. **What is the purpose of the least squares method in Simple Linear Regression?**\n",
        "  - The least squares method minimizes the sum of the squared differences between the observed values and the predicted values.\n",
        "\n",
        "---\n",
        "\n",
        "7. **How is the coefficient of determination (R²) interpreted in Simple Linear Regression?**\n",
        "  - R² represents the proportion of the variance in the dependent variable that is predictable from the independent variable. It ranges from 0 to 1.\n",
        "\n",
        "---\n",
        "\n",
        "8. **What is Multiple Linear Regression?**\n",
        "  - Multiple Linear Regression is an extension of simple linear regression that models the relationship between one dependent variable and two or more independent variables.\n",
        "\n",
        "---\n",
        "\n",
        "9. **What is the main difference between Simple and Multiple Linear Regression?**\n",
        " - Simple Linear Regression involves one independent variable, while Multiple Linear Regression involves two or more independent variables.\n",
        "\n",
        "---\n",
        "\n",
        "10. **What are the key assumptions of Multiple Linear Regression?**\n",
        "   - Linearity\n",
        "   - Independence of errors\n",
        "   - Homoscedasticity\n",
        "   - Normality of errors\n",
        "   - No multicollinearity\n",
        "\n",
        "---\n",
        "\n",
        "11. **What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?**\n",
        "  - Heteroscedasticity occurs when the variance of errors is not constant. It can lead to inefficient estimates and affect the reliability of the model.\n",
        "\n",
        "---\n",
        "\n",
        "12. **How can you improve a Multiple Linear Regression model with high multicollinearity?**\n",
        "  - You can use techniques such as removing highly correlated predictors, using ridge or lasso regression, or applying principal component analysis (PCA).\n",
        "\n",
        "---\n",
        "\n",
        "13. **What are some common techniques for transforming categorical variables for use in regression models?**\n",
        "   - One-Hot Encoding\n",
        "   - Label Encoding\n",
        "   - Dummy Variables\n",
        "\n",
        "---\n",
        "\n",
        "14. **What is the role of interaction terms in Multiple Linear Regression?**\n",
        " -  Interaction terms allow us to capture the combined effect of two or more independent variables on the dependent variable.\n",
        "\n",
        "---\n",
        "\n",
        "15. **How can the interpretation of intercept differ between Simple and Multiple Linear Regression?**\n",
        "  - In Simple Linear Regression, the intercept is the value of Y when X is 0, while in Multiple Linear Regression, it is the expected value of Y when all independent variables are 0.\n",
        "\n",
        "---\n",
        "\n",
        "16. **What is the significance of the slope in regression analysis, and how does it affect predictions?**\n",
        " -  The slope indicates the strength and direction of the relationship between the independent variable and the dependent variable.\n",
        "\n",
        "---\n",
        "\n",
        "17. **How does the intercept in a regression model provide context for the relationship between variables?**\n",
        " -  The intercept represents the baseline value of the dependent variable when all independent variables are zero, providing context for the regression line.\n",
        "\n",
        "---\n",
        "\n",
        "18. **What are the limitations of using R² as a sole measure of model performance?**\n",
        " -  R² can be misleading, especially if there are many predictors in the model. It does not account for overfitting, and it does not measure the model's predictive power.\n",
        "\n",
        "---\n",
        "\n",
        "19. **How would you interpret a large standard error for a regression coefficient?**\n",
        " -  A large standard error indicates that the estimated coefficient is imprecise and there is a high level of uncertainty around it.\n",
        "\n",
        "---\n",
        "\n",
        "20. **How can heteroscedasticity be identified in residual plots, and why is it important to address it?**\n",
        " -  Heteroscedasticity can be identified when the spread of residuals increases or decreases as the fitted values increase. It's important to address it as it affects the validity of hypothesis tests.\n",
        "\n",
        "---\n",
        "\n",
        "21. **What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?**\n",
        " -  A high R² with a low adjusted R² suggests that the model is overfitting the data, possibly due to too many predictors.\n",
        "\n",
        "---\n",
        "\n",
        "22. **Why is it important to scale variables in Multiple Linear Regression?**\n",
        "  - Scaling variables is important to ensure that all variables are on the same scale, which prevents some variables from dominating the model due to their scale.\n",
        "\n",
        "---\n",
        "\n",
        "23. **What is polynomial regression?**\n",
        "  - Polynomial regression is a type of regression analysis where the relationship between the independent and dependent variables is modeled as an nth degree polynomial.\n",
        "\n",
        "---\n",
        "\n",
        "24. **How does polynomial regression differ from linear regression?**\n",
        "  - Polynomial regression models the relationship as a curve (higher-degree polynomial), while linear regression assumes a straight line (degree 1).\n",
        "\n",
        "---\n",
        "\n",
        "25. **When is polynomial regression used?**\n",
        "  - Polynomial regression is used when there is a nonlinear relationship between the independent and dependent variables.\n",
        "\n",
        "---\n",
        "\n",
        "26. **What is the general equation for polynomial regression?**\n",
        "  - The general equation for polynomial regression is:\n",
        "   Y = β0 + β1X + β2X² + ... + βnXⁿ\n",
        "\n",
        "---\n",
        "\n",
        "27. **Can polynomial regression be applied to multiple variables?**\n",
        "  - Yes, polynomial regression can be applied to multiple variables by adding interaction terms between them.\n",
        "\n",
        "---\n",
        "\n",
        "28. **What are the limitations of polynomial regression?**\n",
        "  - The main limitations are the risk of overfitting, high complexity with higher-degree polynomials, and difficulty in interpreting results.\n",
        "\n",
        "---\n",
        "\n",
        "29. **What methods can be used to evaluate model fit when selecting the degree of a polynomial?**\n",
        "  - Methods include cross-validation, adjusted R², and plotting residuals.\n",
        "\n",
        "---\n",
        "\n",
        "30. **Why is visualization important in polynomial regression?**\n",
        "  - Visualization helps to understand the fit of the model and whether the polynomial degree is appropriate for the data.\n",
        "\n",
        "---\n",
        "\n",
        "31. **How is polynomial regression implemented in Python?**\n",
        "  - Polynomial regression can be implemented using `PolynomialFeatures` from sklearn to transform the input features, followed by fitting a linear regression model.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "1M9DOUB9KLAr"
      }
    }
  ]
}